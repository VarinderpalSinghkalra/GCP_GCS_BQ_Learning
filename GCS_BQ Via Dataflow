DATAFLOW ERROR ANALYSIS & FIX â€“ GCS CSV TO BIGQUERY

Project: data-engineering-479617
Dataset: conversational_demo_df
Table: sample_spenddata
Source File: gs://datavip/spend_cube_clean.csv
Runner: DataflowRunner (Batch)

====================================================
ERROR ENCOUNTERED
====================================================
Dataflow job failed during BigQuery write phase with error:

"JSON table encountered too many errors, giving up.
Rows: 1; errors: 1"

The failure occurred during BigQuery file load triggered by
WriteToBigQuery (BigQueryBatchFileLoads).

====================================================
IMPORTANT UNDERSTANDING
====================================================
Although input format is CSV, Apache Beam internally converts records
to JSON files and submits a BigQuery LOAD job.

This means:
- Any invalid record causes BigQuery load failure
- Even a single bad row can fail the entire Dataflow bundle
- Errors are detected only at load time, not during transform

====================================================
ROOT CAUSE
====================================================
At least one row generated by the pipeline did not conform to the
BigQuery table schema.

Common reasons identified:
1. Invalid DATE format in PO_DT
   - Not in YYYY-MM-DD format
   - Empty value or header string passed

2. Invalid NUMERIC value in amt_local
   - Empty string
   - Currency symbols
   - Commas in numbers (e.g. 1,234.56)

3. CSV row column mismatch
   - More or fewer than expected columns
   - Malformed CSV rows

====================================================
WHY DEFAULT PARSING FAILED
====================================================
Initial parsing logic trusted the CSV structure and did not:
- Validate DATE format
- Sanitize numeric fields
- Enforce exact column count
- Drop malformed rows before BigQuery load

As a result, invalid JSON reached BigQuery.

====================================================
FIX IMPLEMENTED
====================================================
Defensive parsing logic was added before writing to BigQuery.

Key fixes:
1. Enforced exact column count match
2. Validated DATE using YYYY-MM-DD format
3. Cleaned numeric field using regex
4. Dropped rows failing validation
5. Prevented invalid JSON from reaching BigQuery

====================================================
UPDATED PIPELINE LOGIC
====================================================
Pipeline steps after fix:
1. Read CSV from GCS
2. Skip header
3. Parse and validate each row
4. Drop invalid rows
5. Write only clean records to BigQuery

====================================================
SERVICE ACCOUNT CONFIGURATION
====================================================
Explicitly configured Dataflow to use custom service account:

sql-bot-sa@data-engineering-479617.iam.gserviceaccount.com

Reason:
- Avoid default compute service account
- Controlled IAM permissions
- Consistent identity across services

====================================================
IAM ROLES REQUIRED
====================================================
roles/dataflow.worker
roles/bigquery.dataEditor
roles/storage.objectViewer

====================================================
VALIDATION AFTER FIX
====================================================
1. Dataflow job completed successfully
2. Verified serviceAccountEmail in Dataflow job details
3. Queried BigQuery table to confirm data load

Command used:
bq query "SELECT COUNT(*) FROM conversational_demo_df.sample_spenddata"

====================================================
KEY LEARNINGS
====================================================
1. Dataflow WriteToBigQuery uses BigQuery LOAD jobs internally.
2. BigQuery is extremely strict with schema enforcement.
3. One bad record can fail an entire Dataflow bundle.
4. CSV parsing must be defensive in production pipelines.
5. Validation should always happen before BigQuery writes.
6. Explicit service account configuration avoids hidden IAM issues.

====================================================
NEXT IMPROVEMENTS
====================================================
- Add dead-letter GCS path for rejected rows
- Add Beam metrics for good vs bad records
- Convert pipeline to Flex Template
- Add structured logging for audit/debugging

====================================================
STATUS
====================================================
Error identified, root cause fixed, pipeline stabilized.
